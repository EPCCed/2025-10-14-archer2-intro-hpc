::: challenge
## Benchmarking the parallel performance

Modify your job script to run on multiple cores and evaluate the performance of `pi-mpi.py`
on a variety of different core counts and use multiple runs to complete a table like the one
below.

If you examine the log file you will see that it contains two timings: 

- Total time taken by the entire program 
- Time taken solely by the calculation i.e. measures only the time spent performing the main computation.. 

The final step, calculation Pi from the Monte-Carlo counts, is not parallelised, meaning it runs on 
a single processor. This creates a serial overhead that does not speed up when more cores are used.

In contracts, the Monte Carlo calculation itself is perfectly parallel *in theory*, since each processor 
works independently on its own set of random numbers. 
Therefore, as you increase the number of cores, this part should ideally run faster.

You can work out **Calculation core** time by multiplying **Calculation time** by the number of cores. 

| Cores      | Overall run time (s) | Calculation time (s) | Calculation core (s) |
|------------|----------------------|----------------------|--------------------------|
| 1 (serial) |                      |                      |                          |
| 2          |                      |                      |                          |
| 4          |                      |                      |                          |
| 8          |                      |                      |                          |
| 16         |                      |                      |                          |
| 32         |                      |                      |                          |
| 64         |                      |                      |                          |
| 128        |                      |                      |                          |
| 256        |                      |                      |                          |

Look at your results – do they make sense? 

Based on how the code is structured, you would expect the calculation performance to scale linearly with 
the number of cores. In other words, as you add more cores, the calculation should finish proportionally faster.

If that’s true, the Calculation core time should stay roughly constant.

Do your results show this expected behaviour?

::: solution

The table below shows example timings for runs on `r config$remote$name`

| Cores      | Overall run time (s) | Calculation time (s) |       Calculation core seconds |
|-----------:|---------------------:|---------------------:|-------------------------------:|
|          1 |                3.931 |                3.854 |                          3.854 |
|          2 |                2.002 |                1.930 |                          3.859 |
|          4 |                1.048 |                0.972 |                          3.888 |
|          8 |                0.572 |                0.495 |                          3.958 |
|         16 |                0.613 |                0.536 |                          8.574 |
|         32 |                0.360 |                0.278 |                          8.880 |
|         64 |                0.249 |                0.163 |                         10.400 |
|        128 |                0.170 |                0.083 |                         10.624 |
|        256 |                0.187 |                0.135 |                         34.560 |

Was our prediction correct?  

:::
:::